# seohasong System
> [우주 최강 강력하고 빈틈없는 라이센스를 따름 무단 사용 걸리면 10억달러 부과 및 지옥행](http://www.bloter.net/archives/209318)

## 줄임말
- https://www.allacronyms.com/

```
There are many studies on how to use various data such as visual expression, voice, and text to improve the performance of emotion recognition algorithms.
Each type of data can be used alone in the classification algorithm, and when used together for analysis, better performance is observed empirically.
This multimodal technique simply uses text and voice related to the speech (Ho et al., 2020),
or additionally uses visual information such as the speaker's expression that occurs at the same time as the speech (Zadeh et al., 2018; Mittal et al., 2020; Delbrouck et al., 2020).
Algorithms that consider multimodality combine data of different properties for one target task.
Among the studies for emotion recognition, there are studies that perform classification by hierarchically combining the relations between modals(Zadeh et al., 2018),
or to select valid features using relations between modals(Mittal et al., 2020).
Attention mechanisms, which are showing good performance in recent years,
have also been used in several studies to find the relationship between modals(Ho et al., 2020; Delbrouck et al., 2020).

Zadeh, A. B., Liang, P. P., Poria, S., Cambria, E., & Morency, L. P. (2018, July).
Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph.
In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 2236-2246).

Mittal, T., Bhattacharya, U., Chandra, R., Bera, A., & Manocha, D. (2020).
M3ER: Multiplicative Multimodal Emotion Recognition using Facial, Textual, and Speech Cues.
In AAAI (pp. 1359-1367).

Ho, N. H., Yang, H. J., Kim, S. H., & Lee, G. (2020).
Multimodal Approach of Speech Emotion Recognition Using Multi-Level Multi-Head Fusion Attention-Based Recurrent Neural Network.
IEEE Access, 8, 61672-61686.

Delbrouck, J. B., Tits, N., Brousmiche, M., & Dupont, S. (2020).
A Transformer-based joint-encoding for Emotion Recognition and Sentiment Analysis.
arXiv preprint arXiv:2006.15955.
```

## DEV
- 자동완성기능

## LEARN
- LLVM
    - https://pdfs.semanticscholar.org/9c64/4f8a2a9135c38edb23437e30aed60a81cbee.pdf?_ga=2.227376664.403027902.1596098548-220327451.1596098548
- https://web.dev/learn/
- typeScript
    - https://github.com/DefinitelyTyped/DefinitelyTyped
- gan cheat
    - https://github.com/soumith/ganhacks
- google
    - https://docs.google.com/document/d/122j9aYgq7HEMA8LKtrgOCb9qFC2klnmTtSctzTHLybs/edit
- js cxx binding
    - node to cpp
    - webAssembly
- python cxx binding
    - https://realpython.com/python-bindings-overview/
    - http://blog.behnel.de/posts/cython-pybind11-cffi-which-tool-to-choose.html

